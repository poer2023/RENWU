# TaskWall v3.0 æŠ€æœ¯æ¶æ„è®¾è®¡æ–‡æ¡£

## æ–‡æ¡£ä¿¡æ¯
- **ç‰ˆæœ¬**: v3.0
- **æ–‡æ¡£ç±»å‹**: æŠ€æœ¯æ¶æ„è®¾è®¡
- **åˆ›å»ºæ—¥æœŸ**: 2025-06-21
- **å…³è”æ–‡æ¡£**: PRD-v3.0-Master.md, PRD-v3.0-AI-Features.md

---

## 1. ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

### 1.1 æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph "å‰ç«¯å±‚ - Frontend Layer"
        A1[Vue 3 åº”ç”¨]
        A2[TypeScript]
        A3[Pinia çŠ¶æ€ç®¡ç†]
        A4[Element Plus UI]
        A5[Canvas æ¸²æŸ“å¼•æ“]
        A6[AI äº¤äº’ç»„ä»¶]
    end
    
    subgraph "APIç½‘å…³å±‚ - API Gateway"
        B1[Nginx åå‘ä»£ç†]
        B2[API è·¯ç”±]
        B3[è´Ÿè½½å‡è¡¡]
        B4[SSL ç»ˆæ­¢]
    end
    
    subgraph "åº”ç”¨æœåŠ¡å±‚ - Application Layer"
        C1[FastAPI ä¸»æœåŠ¡]
        C2[ä»»åŠ¡ç®¡ç†æœåŠ¡]
        C3[AI é›†æˆæœåŠ¡]
        C4[ç”¨æˆ·ç®¡ç†æœåŠ¡]
        C5[é€šçŸ¥æœåŠ¡]
    end
    
    subgraph "AIæœåŠ¡å±‚ - AI Services Layer"
        D1[NLP å¤„ç†æœåŠ¡]
        D2[ä»»åŠ¡åˆ†ç±»æœåŠ¡]
        D3[ç›¸ä¼¼åº¦æ£€æµ‹æœåŠ¡]
        D4[ä¾èµ–æ¨ç†æœåŠ¡]
        D5[ä¼˜å…ˆçº§è¯„ä¼°æœåŠ¡]
        D6[å·¥ä½œé‡ä¼°ç®—æœåŠ¡]
    end
    
    subgraph "æ•°æ®è®¿é—®å±‚ - Data Access Layer"
        E1[SQLAlchemy ORM]
        E2[Redis ç¼“å­˜å±‚]
        E3[å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯]
        E4[æ–‡ä»¶å­˜å‚¨å®¢æˆ·ç«¯]
    end
    
    subgraph "æ•°æ®å­˜å‚¨å±‚ - Data Storage Layer"
        F1[(SQLite ä¸»æ•°æ®åº“)]
        F2[(Redis ç¼“å­˜)]
        F3[(ChromaDB å‘é‡åº“)]
        F4[(æœ¬åœ°æ–‡ä»¶å­˜å‚¨)]
    end
    
    subgraph "å¤–éƒ¨æœåŠ¡ - External Services"
        G1[Google Gemini API]
        G2[OpenAI API - å¤‡é€‰]
        G3[ç›‘æ§æœåŠ¡]
        G4[æ—¥å¿—èšåˆ]
    end
    
    A1 --> B1
    B1 --> C1
    C1 --> D1
    C1 --> E1
    D1 --> G1
    E1 --> F1
    E2 --> F2
    E3 --> F3
    E4 --> F4
```

### 1.2 æŠ€æœ¯æ ˆé€‰æ‹©

#### 1.2.1 å‰ç«¯æŠ€æœ¯æ ˆ
```yaml
æ ¸å¿ƒæ¡†æ¶:
  - Vue 3.4+: ç»„åˆå¼APIï¼Œæ›´å¥½çš„TypeScriptæ”¯æŒ
  - TypeScript 5.0+: ç±»å‹å®‰å…¨ï¼Œå¼€å‘æ•ˆç‡æå‡
  - Vite 5.0+: å¿«é€Ÿæ„å»ºï¼Œçƒ­é‡è½½

çŠ¶æ€ç®¡ç†:
  - Pinia: Vue 3 å®˜æ–¹æ¨èï¼Œç±»å‹å®‰å…¨

UIæ¡†æ¶:
  - Element Plus: æˆç†Ÿçš„Vue 3ç»„ä»¶åº“
  - è‡ªå®šä¹‰ç»„ä»¶: ä»»åŠ¡å¡ç‰‡ã€è¿çº¿æ¸²æŸ“ç­‰

æ„å»ºå·¥å…·:
  - Vite: å¼€å‘æœåŠ¡å™¨å’Œæ„å»ºå·¥å…·
  - ESLint + Prettier: ä»£ç è§„èŒƒ
  - Vitest: å•å…ƒæµ‹è¯•æ¡†æ¶
```

#### 1.2.2 åç«¯æŠ€æœ¯æ ˆ
```yaml
Webæ¡†æ¶:
  - FastAPI: é«˜æ€§èƒ½ï¼Œè‡ªåŠ¨APIæ–‡æ¡£ï¼Œå¼‚æ­¥æ”¯æŒ
  - Uvicorn: ASGIæœåŠ¡å™¨

æ•°æ®åº“:
  - SQLite: ä¸»æ•°æ®åº“ï¼Œç®€å•éƒ¨ç½²
  - SQLAlchemy: ORMæ¡†æ¶
  - Alembic: æ•°æ®åº“è¿ç§»

AI/ML:
  - Google Gemini API: ä¸»è¦NLPæœåŠ¡
  - sentence-transformers: æœ¬åœ°å‘é‡åŒ–
  - scikit-learn: æœºå™¨å­¦ä¹ ç®—æ³•
  - ChromaDB: å‘é‡æ•°æ®åº“

ç¼“å­˜:
  - Redis: ç¼“å­˜ï¼Œä¼šè¯å­˜å‚¨

éƒ¨ç½²:
  - Docker: å®¹å™¨åŒ–éƒ¨ç½²
  - Docker Compose: å¤šæœåŠ¡ç¼–æ’
```

---

## 2. æ•°æ®åº“è®¾è®¡

### 2.1 æ•°æ®æ¨¡å‹è®¾è®¡

#### 2.1.1 æ ¸å¿ƒæ•°æ®æ¨¡å‹
```python
from sqlmodel import SQLModel, Field, Relationship
from datetime import datetime
from typing import Optional, List
from enum import Enum

class PriorityLevel(int, Enum):
    CRITICAL = 0    # ç´§æ€¥
    HIGH = 1        # é«˜
    MEDIUM = 2      # ä¸­
    LOW = 3         # ä½
    BACKLOG = 4     # å¾…åŠ

class TaskStatus(str, Enum):
    TODO = "todo"
    IN_PROGRESS = "in_progress"
    DONE = "done"
    ARCHIVED = "archived"

class DependencyType(str, Enum):
    BLOCKS = "blocks"           # é˜»å¡å…³ç³»
    ENABLES = "enables"         # å¯ç”¨å…³ç³»
    RELATES = "relates"         # ç›¸å…³å…³ç³»
    SUBTASK = "subtask"         # å­ä»»åŠ¡å…³ç³»
    RESOURCE_SHARED = "shared"  # èµ„æºå…±äº«

# ä»»åŠ¡è¡¨
class Task(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    title: str = Field(max_length=200, index=True)
    description: str = Field(default="")
    priority: PriorityLevel = Field(default=PriorityLevel.MEDIUM)
    status: TaskStatus = Field(default=TaskStatus.TODO)
    
    # åˆ†ç±»ä¿¡æ¯
    module_id: Optional[int] = Field(foreign_key="module.id")
    category: Optional[str] = Field(max_length=50)
    tags: Optional[str] = Field(default="")  # JSONå­—ç¬¦ä¸²å­˜å‚¨æ ‡ç­¾
    
    # å±‚æ¬¡ç»“æ„
    parent_id: Optional[int] = Field(foreign_key="task.id")
    
    # æ—¶é—´ä¿¡æ¯
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    deadline: Optional[datetime] = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    
    # å·¥ä½œé‡ä¼°ç®—
    estimated_hours: float = Field(default=0.0)
    actual_hours: float = Field(default=0.0)
    
    # ä½ç½®ä¿¡æ¯ï¼ˆç”»å¸ƒåæ ‡ï¼‰
    position_x: float = Field(default=0.0)
    position_y: float = Field(default=0.0)
    
    # AIç›¸å…³å­—æ®µ
    ai_generated: bool = Field(default=False)
    ai_confidence: float = Field(default=0.0)
    ai_reasoning: Optional[str] = None
    
    # å…³è”å…³ç³»
    module: Optional["Module"] = Relationship(back_populates="tasks")
    parent: Optional["Task"] = Relationship(back_populates="children", sa_relationship_kwargs={"remote_side": "Task.id"})
    children: List["Task"] = Relationship(back_populates="parent")
    history: List["TaskHistory"] = Relationship(back_populates="task")
    
    # ä¾èµ–å…³ç³»
    depends_on: List["TaskDependency"] = Relationship(back_populates="from_task", sa_relationship_kwargs={"foreign_keys": "TaskDependency.from_task_id"})
    dependents: List["TaskDependency"] = Relationship(back_populates="to_task", sa_relationship_kwargs={"foreign_keys": "TaskDependency.to_task_id"})

# æ¨¡å—è¡¨
class Module(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    name: str = Field(max_length=100, unique=True)
    description: str = Field(default="")
    color: str = Field(max_length=7, default="#3B82F6")  # åå…­è¿›åˆ¶é¢œè‰²
    icon: str = Field(max_length=10, default="ğŸ“")
    
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    
    # å…³è”å…³ç³»
    tasks: List[Task] = Relationship(back_populates="module")

# ä»»åŠ¡ä¾èµ–å…³ç³»è¡¨
class TaskDependency(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    from_task_id: int = Field(foreign_key="task.id")
    to_task_id: int = Field(foreign_key="task.id")
    dependency_type: DependencyType = Field(default=DependencyType.BLOCKS)
    
    # AIæ¨ç†ç›¸å…³
    ai_inferred: bool = Field(default=False)
    confidence: float = Field(default=1.0)
    reasoning: Optional[str] = None
    
    created_at: datetime = Field(default_factory=datetime.utcnow)
    
    # å…³è”å…³ç³»
    from_task: Task = Relationship(back_populates="depends_on", sa_relationship_kwargs={"foreign_keys": "TaskDependency.from_task_id"})
    to_task: Task = Relationship(back_populates="dependents", sa_relationship_kwargs={"foreign_keys": "TaskDependency.to_task_id"})

# ä»»åŠ¡å†å²è®°å½•è¡¨
class TaskHistory(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    task_id: int = Field(foreign_key="task.id")
    field: str = Field(max_length=50)  # å˜æ›´çš„å­—æ®µå
    old_value: Optional[str] = None
    new_value: Optional[str] = None
    change_type: str = Field(max_length=20)  # create, update, delete
    
    # å˜æ›´æ¥æº
    source: str = Field(default="user")  # user, ai, system
    user_id: Optional[str] = None
    
    created_at: datetime = Field(default_factory=datetime.utcnow)
    
    # å…³è”å…³ç³»
    task: Task = Relationship(back_populates="history")

# AIä»»åŠ¡å‘é‡è¡¨ï¼ˆç”¨äºç›¸ä¼¼åº¦æ£€æµ‹ï¼‰
class TaskVector(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    task_id: int = Field(foreign_key="task.id", unique=True)
    vector_id: str = Field(max_length=100)  # ChromaDBä¸­çš„å‘é‡ID
    last_updated: datetime = Field(default_factory=datetime.utcnow)

# ç”¨æˆ·åé¦ˆè¡¨
class AIFeedback(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    operation: str = Field(max_length=50)  # æ“ä½œç±»å‹ï¼šparse, classify, etc.
    input_hash: str = Field(max_length=64)  # è¾“å…¥æ•°æ®çš„å“ˆå¸Œ
    ai_result: str = Field()  # AIç»“æœçš„JSON
    user_correction: Optional[str] = None  # ç”¨æˆ·ä¿®æ­£çš„JSON
    feedback_type: str = Field(max_length=20)  # accept, reject, modify
    
    created_at: datetime = Field(default_factory=datetime.utcnow)
```

#### 2.1.2 æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–
```sql
-- æ€§èƒ½ä¼˜åŒ–ç´¢å¼•
CREATE INDEX idx_task_status_priority ON task(status, priority);
CREATE INDEX idx_task_module_created ON task(module_id, created_at);
CREATE INDEX idx_task_deadline ON task(deadline) WHERE deadline IS NOT NULL;
CREATE INDEX idx_task_parent ON task(parent_id) WHERE parent_id IS NOT NULL;

-- ä¾èµ–å…³ç³»æŸ¥è¯¢ä¼˜åŒ–
CREATE INDEX idx_dependency_from ON taskdependency(from_task_id);
CREATE INDEX idx_dependency_to ON taskdependency(to_task_id);

-- å†å²è®°å½•æŸ¥è¯¢ä¼˜åŒ–
CREATE INDEX idx_history_task_time ON taskhistory(task_id, created_at);

-- å…¨æ–‡æœç´¢ç´¢å¼•
CREATE VIRTUAL TABLE task_fts USING fts5(
    title, description, content=task, 
    tokenize = 'porter unicode61'
);
```

### 2.2 å‘é‡æ•°æ®åº“è®¾è®¡

#### 2.2.1 ChromaDBé›†åˆè®¾è®¡
```python
import chromadb
from chromadb.config import Settings

class VectorDBManager:
    def __init__(self):
        self.client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory="./data/chroma"
        ))
        
        # ä»»åŠ¡å‘é‡é›†åˆ
        self.task_collection = self.client.get_or_create_collection(
            name="task_vectors",
            metadata={
                "description": "Task content vectors for similarity search",
                "version": "1.0"
            }
        )
        
        # åˆ†ç±»å‘é‡é›†åˆ
        self.category_collection = self.client.get_or_create_collection(
            name="category_vectors", 
            metadata={
                "description": "Category vectors for classification",
                "version": "1.0"
            }
        )
    
    def add_task_vector(self, task_id: int, content: str, metadata: dict):
        """æ·»åŠ ä»»åŠ¡å‘é‡"""
        from sentence_transformers import SentenceTransformer
        
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        embedding = model.encode(content).tolist()
        
        self.task_collection.add(
            ids=[f"task_{task_id}"],
            embeddings=[embedding],
            documents=[content],
            metadatas=[{
                "task_id": task_id,
                "title": metadata.get("title", ""),
                "category": metadata.get("category", ""),
                "priority": metadata.get("priority", 2),
                "created_at": metadata.get("created_at", "")
            }]
        )
    
    def find_similar_tasks(self, content: str, n_results: int = 5, threshold: float = 0.8):
        """æŸ¥æ‰¾ç›¸ä¼¼ä»»åŠ¡"""
        from sentence_transformers import SentenceTransformer
        
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        query_embedding = model.encode(content).tolist()
        
        results = self.task_collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            include=["documents", "metadatas", "distances"]
        )
        
        # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
        filtered_results = []
        for i, distance in enumerate(results["distances"][0]):
            similarity = 1 - distance  # ChromaDBä½¿ç”¨è·ç¦»ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦
            if similarity >= threshold:
                filtered_results.append({
                    "task_id": results["metadatas"][0][i]["task_id"],
                    "similarity": similarity,
                    "content": results["documents"][0][i],
                    "metadata": results["metadatas"][0][i]
                })
        
        return filtered_results
```

---

## 3. APIè®¾è®¡

### 3.1 RESTful APIè®¾è®¡

#### 3.1.1 APIè·¯ç”±ç»“æ„
```python
from fastapi import FastAPI, APIRouter
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(
    title="TaskWall API",
    description="AI-powered task management system",
    version="3.0.0"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è·¯ç”±æ¨¡å—
api_router = APIRouter(prefix="/api")

# ä»»åŠ¡ç®¡ç†è·¯ç”±
tasks_router = APIRouter(prefix="/tasks", tags=["tasks"])
modules_router = APIRouter(prefix="/modules", tags=["modules"])
dependencies_router = APIRouter(prefix="/dependencies", tags=["dependencies"])

# AIåŠŸèƒ½è·¯ç”±
ai_router = APIRouter(prefix="/ai", tags=["ai"])
nlp_router = APIRouter(prefix="/nlp", tags=["nlp"])
analysis_router = APIRouter(prefix="/analysis", tags=["analysis"])

# åŒ…å«è·¯ç”±
api_router.include_router(tasks_router)
api_router.include_router(modules_router)
api_router.include_router(dependencies_router)
api_router.include_router(ai_router)
api_router.include_router(nlp_router)
api_router.include_router(analysis_router)

app.include_router(api_router)
```

#### 3.1.2 æ ¸å¿ƒAPIç«¯ç‚¹è®¾è®¡
```python
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime

# è¯·æ±‚/å“åº”æ¨¡å‹
class TaskCreateRequest(BaseModel):
    title: str
    description: Optional[str] = ""
    priority: Optional[int] = 2
    module_id: Optional[int] = None
    parent_id: Optional[int] = None
    deadline: Optional[datetime] = None
    estimated_hours: Optional[float] = 0.0
    position_x: Optional[float] = 0.0
    position_y: Optional[float] = 0.0

class TaskResponse(BaseModel):
    id: int
    title: str
    description: str
    priority: int
    status: str
    module_id: Optional[int]
    parent_id: Optional[int]
    created_at: datetime
    updated_at: datetime
    deadline: Optional[datetime]
    estimated_hours: float
    actual_hours: float
    position_x: float
    position_y: float
    ai_confidence: float

# AIç›¸å…³è¯·æ±‚æ¨¡å‹
class NLPParseRequest(BaseModel):
    text: str
    context: Optional[dict] = None

class NLPParseResponse(BaseModel):
    tasks: List[dict]
    confidence: float
    reasoning: List[str]

class TaskClassificationRequest(BaseModel):
    task_ids: List[int]
    user_context: Optional[dict] = None

class SimilaritySearchRequest(BaseModel):
    task_id: int
    threshold: Optional[float] = 0.8
    max_results: Optional[int] = 5

# APIç«¯ç‚¹å®ç°
@tasks_router.post("/", response_model=TaskResponse)
async def create_task(task: TaskCreateRequest, db: Session = Depends(get_db)):
    """åˆ›å»ºæ–°ä»»åŠ¡"""
    db_task = Task(**task.dict())
    db.add(db_task)
    db.commit()
    db.refresh(db_task)
    
    # å¼‚æ­¥å¤„ç†AIå¢å¼º
    background_tasks.add_task(enhance_task_with_ai, db_task.id)
    
    return db_task

@tasks_router.get("/", response_model=List[TaskResponse])
async def list_tasks(
    skip: int = 0,
    limit: int = 100,
    status: Optional[str] = None,
    module_id: Optional[int] = None,
    db: Session = Depends(get_db)
):
    """è·å–ä»»åŠ¡åˆ—è¡¨"""
    query = db.query(Task)
    
    if status:
        query = query.filter(Task.status == status)
    if module_id:
        query = query.filter(Task.module_id == module_id)
    
    tasks = query.offset(skip).limit(limit).all()
    return tasks

@ai_router.post("/parse", response_model=NLPParseResponse)
async def parse_natural_language(
    request: NLPParseRequest,
    ai_service: AIService = Depends(get_ai_service)
):
    """è‡ªç„¶è¯­è¨€ä»»åŠ¡è§£æ"""
    try:
        result = await ai_service.parse_tasks_from_text(
            text=request.text,
            context=request.context
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@ai_router.post("/classify", response_model=List[dict])
async def classify_tasks(
    request: TaskClassificationRequest,
    ai_service: AIService = Depends(get_ai_service)
):
    """ä»»åŠ¡æ™ºèƒ½åˆ†ç±»"""
    try:
        result = await ai_service.classify_tasks(
            task_ids=request.task_ids,
            user_context=request.user_context
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@ai_router.post("/similar", response_model=List[dict])
async def find_similar_tasks(
    request: SimilaritySearchRequest,
    ai_service: AIService = Depends(get_ai_service)
):
    """æŸ¥æ‰¾ç›¸ä¼¼ä»»åŠ¡"""
    try:
        result = await ai_service.find_similar_tasks(
            task_id=request.task_id,
            threshold=request.threshold,
            max_results=request.max_results
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### 3.2 WebSocketå®æ—¶é€šä¿¡

#### 3.2.1 WebSocketè¿æ¥ç®¡ç†
```python
from fastapi import WebSocket, WebSocketDisconnect
import json
import asyncio

class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.user_connections: dict = {}
    
    async def connect(self, websocket: WebSocket, user_id: str):
        await websocket.accept()
        self.active_connections.append(websocket)
        self.user_connections[user_id] = websocket
    
    def disconnect(self, websocket: WebSocket, user_id: str):
        self.active_connections.remove(websocket)
        if user_id in self.user_connections:
            del self.user_connections[user_id]
    
    async def send_personal_message(self, message: str, user_id: str):
        if user_id in self.user_connections:
            await self.user_connections[user_id].send_text(message)
    
    async def broadcast(self, message: str):
        for connection in self.active_connections:
            await connection.send_text(message)

manager = ConnectionManager()

@app.websocket("/ws/{user_id}")
async def websocket_endpoint(websocket: WebSocket, user_id: str):
    await manager.connect(websocket, user_id)
    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            # å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯
            if message["type"] == "task_update":
                await handle_task_update(message, user_id)
            elif message["type"] == "ai_request":
                await handle_ai_request(message, user_id)
            
    except WebSocketDisconnect:
        manager.disconnect(websocket, user_id)

async def handle_ai_request(message: dict, user_id: str):
    """å¤„ç†AIè¯·æ±‚"""
    try:
        # å¤„ç†AIè¯·æ±‚
        result = await process_ai_request(message)
        
        # å‘é€ç»“æœç»™ç”¨æˆ·
        response = {
            "type": "ai_response",
            "request_id": message.get("request_id"),
            "result": result
        }
        await manager.send_personal_message(json.dumps(response), user_id)
        
    except Exception as e:
        error_response = {
            "type": "ai_error",
            "request_id": message.get("request_id"),
            "error": str(e)
        }
        await manager.send_personal_message(json.dumps(error_response), user_id)
```

---

## 4. AIæœåŠ¡æ¶æ„

### 4.1 AIæœåŠ¡æ¨¡å—è®¾è®¡

#### 4.1.1 AIæœåŠ¡åŸºç¡€æ¶æ„
```python
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
import asyncio
from datetime import datetime

class AIServiceBase(ABC):
    """AIæœåŠ¡åŸºç±»"""
    
    def __init__(self):
        self.cache = AIResultCache()
        self.monitor = AIMonitor()
    
    @abstractmethod
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†AIè¯·æ±‚çš„æŠ½è±¡æ–¹æ³•"""
        pass
    
    async def process_with_cache(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¸¦ç¼“å­˜çš„å¤„ç†æ–¹æ³•"""
        # 1. ç”Ÿæˆè¾“å…¥å“ˆå¸Œ
        input_hash = self._generate_input_hash(input_data)
        
        # 2. æ£€æŸ¥ç¼“å­˜
        cached_result = await self.cache.get_cached_result(
            operation=self.__class__.__name__,
            input_hash=input_hash
        )
        
        if cached_result:
            return cached_result
        
        # 3. æ‰§è¡ŒAIå¤„ç†
        start_time = datetime.now()
        try:
            result = await self.process(input_data)
            execution_time = (datetime.now() - start_time).total_seconds()
            
            # 4. ç¼“å­˜ç»“æœ
            await self.cache.cache_result(
                operation=self.__class__.__name__,
                input_hash=input_hash,
                result=result
            )
            
            # 5. è®°å½•ç›‘æ§æŒ‡æ ‡
            self.monitor.track_ai_operation(
                operation=self.__class__.__name__,
                input_data=input_data,
                result=result,
                execution_time=execution_time
            )
            
            return result
            
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            self.monitor.track_ai_error(
                operation=self.__class__.__name__,
                input_data=input_data,
                error=str(e),
                execution_time=execution_time
            )
            raise

class NLPParsingService(AIServiceBase):
    """è‡ªç„¶è¯­è¨€è§£ææœåŠ¡"""
    
    def __init__(self):
        super().__init__()
        self.gemini_client = GeminiClient()
        self.parser = TaskNLPParser()
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        text = input_data["text"]
        context = input_data.get("context", {})
        
        # 1. é¢„å¤„ç†æ–‡æœ¬
        cleaned_text = self.parser.preprocess_text(text)
        
        # 2. è°ƒç”¨Gemini APIè§£æ
        parsing_result = await self.gemini_client.parse_tasks(
            text=cleaned_text,
            context=context
        )
        
        # 3. åå¤„ç†å’ŒéªŒè¯
        validated_tasks = self.parser.validate_parsed_tasks(parsing_result["tasks"])
        
        return {
            "tasks": validated_tasks,
            "confidence": parsing_result["confidence"],
            "reasoning": parsing_result["reasoning"]
        }

class TaskClassificationService(AIServiceBase):
    """ä»»åŠ¡åˆ†ç±»æœåŠ¡"""
    
    def __init__(self):
        super().__init__()
        self.classifier = TaskClassifier()
        self.vector_db = VectorDBManager()
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        task_ids = input_data["task_ids"]
        user_context = input_data.get("user_context", {})
        
        results = []
        for task_id in task_ids:
            # 1. è·å–ä»»åŠ¡æ•°æ®
            task = await self._get_task(task_id)
            
            # 2. æ‰§è¡Œåˆ†ç±»
            classification = await self.classifier.classify_task(task, user_context)
            
            # 3. æ›´æ–°å‘é‡æ•°æ®åº“
            await self.vector_db.update_task_vector(task_id, task, classification)
            
            results.append({
                "task_id": task_id,
                "classification": classification
            })
        
        return {"results": results}

class SimilarityDetectionService(AIServiceBase):
    """ç›¸ä¼¼åº¦æ£€æµ‹æœåŠ¡"""
    
    def __init__(self):
        super().__init__()
        self.vector_db = VectorDBManager()
        self.similarity_analyzer = SimilarityAnalyzer()
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        task_id = input_data["task_id"]
        threshold = input_data.get("threshold", 0.8)
        max_results = input_data.get("max_results", 5)
        
        # 1. è·å–ä»»åŠ¡å†…å®¹
        task = await self._get_task(task_id)
        task_content = f"{task.title} {task.description}"
        
        # 2. å‘é‡æœç´¢
        similar_tasks = await self.vector_db.find_similar_tasks(
            content=task_content,
            n_results=max_results,
            threshold=threshold
        )
        
        # 3. è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æ
        detailed_similarities = []
        for similar_task in similar_tasks:
            detailed_sim = await self.similarity_analyzer.analyze_detailed_similarity(
                task, similar_task
            )
            detailed_similarities.append(detailed_sim)
        
        return {"similar_tasks": detailed_similarities}
```

#### 4.1.2 AIæœåŠ¡èšåˆå™¨
```python
class AIServiceAggregator:
    """AIæœåŠ¡èšåˆå™¨ï¼Œåè°ƒå¤šä¸ªAIæœåŠ¡"""
    
    def __init__(self):
        self.nlp_service = NLPParsingService()
        self.classification_service = TaskClassificationService()
        self.similarity_service = SimilarityDetectionService()
        self.priority_service = PriorityEstimationService()
        self.dependency_service = DependencyInferenceService()
    
    async def process_new_task_ai_pipeline(self, task_data: dict) -> dict:
        """æ–°ä»»åŠ¡çš„AIå¤„ç†æµæ°´çº¿"""
        try:
            # 1. å¦‚æœæ˜¯è‡ªç„¶è¯­è¨€è¾“å…¥ï¼Œå…ˆè§£æ
            if task_data.get("is_natural_language"):
                parse_result = await self.nlp_service.process_with_cache({
                    "text": task_data["raw_text"],
                    "context": task_data.get("context", {})
                })
                task_data.update(parse_result["tasks"][0])
            
            # 2. ä»»åŠ¡åˆ†ç±»
            classification_result = await self.classification_service.process_with_cache({
                "task_data": task_data,
                "user_context": task_data.get("user_context", {})
            })
            
            # 3. ç›¸ä¼¼åº¦æ£€æµ‹ï¼ˆæŸ¥æ‰¾é‡å¤ä»»åŠ¡ï¼‰
            if task_data.get("check_duplicates", True):
                similarity_result = await self.similarity_service.process_with_cache({
                    "task_content": f"{task_data['title']} {task_data.get('description', '')}",
                    "threshold": 0.85
                })
            
            # 4. ä¼˜å…ˆçº§è¯„ä¼°
            priority_result = await self.priority_service.process_with_cache({
                "task_data": task_data,
                "context": task_data.get("context", {})
            })
            
            # 5. ä¾èµ–å…³ç³»æ¨ç†ï¼ˆå¦‚æœæœ‰å…¶ä»–ä»»åŠ¡ï¼‰
            if task_data.get("infer_dependencies", True):
                dependency_result = await self.dependency_service.process_with_cache({
                    "new_task": task_data,
                    "existing_tasks": task_data.get("existing_tasks", [])
                })
            
            # 6. èšåˆç»“æœ
            ai_enhancement = {
                "classification": classification_result,
                "similar_tasks": similarity_result.get("similar_tasks", []),
                "priority_suggestion": priority_result,
                "suggested_dependencies": dependency_result.get("dependencies", []),
                "ai_confidence": self._calculate_overall_confidence([
                    classification_result.get("confidence", 0),
                    priority_result.get("confidence", 0)
                ])
            }
            
            return ai_enhancement
            
        except Exception as e:
            # AIå¢å¼ºå¤±è´¥ä¸åº”è¯¥é˜»æ­¢ä»»åŠ¡åˆ›å»º
            print(f"AI pipeline error: {e}")
            return {"error": str(e), "ai_confidence": 0.0}
    
    def _calculate_overall_confidence(self, confidences: List[float]) -> float:
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦"""
        if not confidences:
            return 0.0
        return sum(confidences) / len(confidences)
```

### 4.2 å¤–éƒ¨AIæœåŠ¡é›†æˆ

#### 4.2.1 Google Gemini APIé›†æˆ
```python
import google.generativeai as genai
from typing import Dict, List, Any
import json
import asyncio

class GeminiClient:
    """Google Gemini APIå®¢æˆ·ç«¯"""
    
    def __init__(self):
        genai.configure(api_key=settings.GEMINI_API_KEY)
        self.model = genai.GenerativeModel('gemini-pro')
        
        # ä»»åŠ¡è§£æçš„æç¤ºæ¨¡æ¿
        self.task_parsing_prompt = """
        è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œæå–å…¶ä¸­çš„ä»»åŠ¡ä¿¡æ¯ã€‚è¿”å›JSONæ ¼å¼çš„ä»»åŠ¡åˆ—è¡¨ã€‚

        æ–‡æœ¬: {text}

        æ¯ä¸ªä»»åŠ¡åº”åŒ…å«ä»¥ä¸‹å­—æ®µ:
        - title: ä»»åŠ¡æ ‡é¢˜ (å¿…éœ€)
        - description: è¯¦ç»†æè¿° (å¯é€‰)
        - priority: ä¼˜å…ˆçº§ 0-4 (0=ç´§æ€¥, 1=é«˜, 2=ä¸­, 3=ä½, 4=å¾…åŠ)
        - deadline: æˆªæ­¢æ—¶é—´ (ISOæ ¼å¼ï¼Œå¯é€‰)
        - estimated_hours: é¢„ä¼°å·¥æ—¶ (æ•°å­—ï¼Œå¯é€‰)
        - tags: æ ‡ç­¾åˆ—è¡¨ (å¯é€‰)

        è¯·ç¡®ä¿:
        1. å‡†ç¡®è¯†åˆ«ä»»åŠ¡è¾¹ç•Œ
        2. ä»ä¸Šä¸‹æ–‡æ¨æ–­ä¼˜å…ˆçº§
        3. è¯†åˆ«æ—¶é—´è¡¨è¾¾å¼
        4. æå–å…³é”®æ ‡ç­¾

        è¿”å›æ ¼å¼:
        {
            "tasks": [...],
            "confidence": 0.85,
            "reasoning": ["è§£æä¾æ®1", "è§£æä¾æ®2"]
        }
        """
    
    async def parse_tasks(self, text: str, context: dict = None) -> dict:
        """è§£æä»»åŠ¡æ–‡æœ¬"""
        try:
            # æ„å»ºæç¤º
            prompt = self.task_parsing_prompt.format(text=text)
            if context:
                prompt += f"\n\nä¸Šä¸‹æ–‡ä¿¡æ¯: {json.dumps(context, ensure_ascii=False)}"
            
            # è°ƒç”¨Gemini API
            response = await asyncio.to_thread(
                self.model.generate_content, prompt
            )
            
            # è§£æå“åº”
            result_text = response.text
            
            # å°è¯•è§£æJSON
            try:
                result = json.loads(result_text)
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯æ ‡å‡†JSONï¼Œå°è¯•æå–JSONéƒ¨åˆ†
                result = self._extract_json_from_text(result_text)
            
            return self._validate_parsing_result(result)
            
        except Exception as e:
            print(f"Gemini API error: {e}")
            return {
                "tasks": [],
                "confidence": 0.0,
                "reasoning": [f"APIè°ƒç”¨å¤±è´¥: {str(e)}"]
            }
    
    def _extract_json_from_text(self, text: str) -> dict:
        """ä»æ–‡æœ¬ä¸­æå–JSON"""
        import re
        
        # æŸ¥æ‰¾JSONå—
        json_pattern = r'```json\s*(\{.*?\})\s*```'
        match = re.search(json_pattern, text, re.DOTALL)
        
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass
        
        # æŸ¥æ‰¾æ™®é€šçš„JSONå¯¹è±¡
        json_pattern = r'(\{.*\})'
        match = re.search(json_pattern, text, re.DOTALL)
        
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass
        
        # è§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æ„
        return {
            "tasks": [],
            "confidence": 0.0,
            "reasoning": ["æ— æ³•è§£æGeminiå“åº”ä¸­çš„JSON"]
        }
    
    def _validate_parsing_result(self, result: dict) -> dict:
        """éªŒè¯è§£æç»“æœ"""
        if not isinstance(result, dict):
            return {"tasks": [], "confidence": 0.0, "reasoning": ["ç»“æœæ ¼å¼é”™è¯¯"]}
        
        # ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
        if "tasks" not in result:
            result["tasks"] = []
        
        if "confidence" not in result:
            result["confidence"] = 0.5
        
        if "reasoning" not in result:
            result["reasoning"] = ["è‡ªåŠ¨ç”Ÿæˆçš„è§£æç»“æœ"]
        
        # éªŒè¯æ¯ä¸ªä»»åŠ¡
        validated_tasks = []
        for task in result["tasks"]:
            if isinstance(task, dict) and "title" in task:
                # ç¡®ä¿ä¼˜å…ˆçº§åœ¨æœ‰æ•ˆèŒƒå›´å†…
                if "priority" in task:
                    task["priority"] = max(0, min(4, int(task.get("priority", 2))))
                else:
                    task["priority"] = 2
                
                validated_tasks.append(task)
        
        result["tasks"] = validated_tasks
        return result
```

#### 4.2.2 å¤‡é€‰AIæœåŠ¡ï¼ˆOpenAIï¼‰
```python
import openai
from typing import Dict, List, Any

class OpenAIClient:
    """OpenAI APIå®¢æˆ·ç«¯ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰"""
    
    def __init__(self):
        openai.api_key = settings.OPENAI_API_KEY
        self.model = "gpt-4"
    
    async def parse_tasks(self, text: str, context: dict = None) -> dict:
        """ä½¿ç”¨OpenAI GPT-4è§£æä»»åŠ¡"""
        try:
            system_prompt = """
            ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡ç®¡ç†åŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ï¼Œæå–å…¶ä¸­çš„ä»»åŠ¡ä¿¡æ¯ã€‚
            
            è¿”å›æ ¼å¼è¦æ±‚ï¼š
            - ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›
            - åŒ…å«tasksæ•°ç»„ã€confidenceåˆ†æ•°å’Œreasoningæ•°ç»„
            - æ¯ä¸ªä»»åŠ¡å¿…é¡»æœ‰titleå­—æ®µ
            - ä¼˜å…ˆçº§èŒƒå›´0-4ï¼ˆ0=ç´§æ€¥ï¼Œ4=å¾…åŠï¼‰
            """
            
            user_prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬å¹¶æå–ä»»åŠ¡ï¼š\n\n{text}"
            if context:
                user_prompt += f"\n\nä¸Šä¸‹æ–‡ï¼š{json.dumps(context, ensure_ascii=False)}"
            
            response = await openai.ChatCompletion.acreate(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            result_text = response.choices[0].message.content
            result = json.loads(result_text)
            
            return self._validate_parsing_result(result)
            
        except Exception as e:
            print(f"OpenAI API error: {e}")
            return {
                "tasks": [],
                "confidence": 0.0,
                "reasoning": [f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}"]
            }
```

---

## 5. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 5.1 å‰ç«¯æ€§èƒ½ä¼˜åŒ–

#### 5.1.1 ç»„ä»¶æ‡’åŠ è½½
```typescript
// è·¯ç”±æ‡’åŠ è½½
const routes = [
  {
    path: '/',
    name: 'Home',
    component: () => import('@/pages/Home.vue')
  },
  {
    path: '/timeline',
    name: 'Timeline', 
    component: () => import('@/pages/Timeline.vue')
  },
  {
    path: '/analytics',
    name: 'Analytics',
    component: () => import('@/pages/Analytics.vue')
  }
]

// ç»„ä»¶æ‡’åŠ è½½
export default defineAsyncComponent({
  loader: () => import('./TaskCard.vue'),
  loadingComponent: TaskCardSkeleton,
  errorComponent: TaskCardError,
  delay: 200,
  timeout: 3000
})
```

#### 5.1.2 è™šæ‹Ÿæ»šåŠ¨
```vue
<template>
  <div class="virtual-list-container">
    <RecycleScroller
      class="scroller"
      :items="tasks"
      :item-size="120"
      key-field="id"
      v-slot="{ item }"
    >
      <TaskCard :task="item" />
    </RecycleScroller>
  </div>
</template>

<script setup lang="ts">
import { RecycleScroller } from 'vue-virtual-scroller'

const props = defineProps<{
  tasks: Task[]
}>()
</script>
```

#### 5.1.3 çŠ¶æ€ç®¡ç†ä¼˜åŒ–
```typescript
// PiniaçŠ¶æ€ä¼˜åŒ–
export const useTaskStore = defineStore('tasks', () => {
  const tasks = ref<Task[]>([])
  const tasksMap = computed(() => {
    const map = new Map<number, Task>()
    tasks.value.forEach(task => map.set(task.id, task))
    return map
  })
  
  // ä½¿ç”¨Mapæé«˜æŸ¥æ‰¾æ€§èƒ½
  const getTaskById = (id: number) => tasksMap.value.get(id)
  
  // åˆ†é¡µåŠ è½½
  const loadTasks = async (page: number = 1, limit: number = 50) => {
    const response = await api.get(`/tasks?page=${page}&limit=${limit}`)
    
    if (page === 1) {
      tasks.value = response.data
    } else {
      tasks.value.push(...response.data)
    }
  }
  
  // æ™ºèƒ½æ›´æ–°ï¼ˆé¿å…é‡å¤æ¸²æŸ“ï¼‰
  const updateTask = (updatedTask: Task) => {
    const index = tasks.value.findIndex(t => t.id === updatedTask.id)
    if (index !== -1) {
      // ä½¿ç”¨Object.assigné¿å…å“åº”å¼é‡æ–°åˆ›å»º
      Object.assign(tasks.value[index], updatedTask)
    }
  }
  
  return {
    tasks: readonly(tasks),
    tasksMap,
    getTaskById,
    loadTasks,
    updateTask
  }
})
```

### 5.2 åç«¯æ€§èƒ½ä¼˜åŒ–

#### 5.2.1 æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–
```python
from sqlalchemy.orm import selectinload, joinedload
from sqlalchemy import and_, or_

class TaskRepository:
    def __init__(self, db: Session):
        self.db = db
    
    async def get_tasks_with_relations(self, filters: dict = None) -> List[Task]:
        """ä¼˜åŒ–çš„ä»»åŠ¡æŸ¥è¯¢ï¼Œé¢„åŠ è½½å…³è”æ•°æ®"""
        query = self.db.query(Task).options(
            selectinload(Task.module),
            selectinload(Task.children),
            selectinload(Task.depends_on).selectinload(TaskDependency.to_task),
            selectinload(Task.dependents).selectinload(TaskDependency.from_task)
        )
        
        # åŠ¨æ€æ„å»ºè¿‡æ»¤æ¡ä»¶
        if filters:
            if filters.get('status'):
                query = query.filter(Task.status == filters['status'])
            if filters.get('module_id'):
                query = query.filter(Task.module_id == filters['module_id'])
            if filters.get('priority_range'):
                min_p, max_p = filters['priority_range']
                query = query.filter(and_(Task.priority >= min_p, Task.priority <= max_p))
        
        return query.all()
    
    async def search_tasks(self, search_term: str, limit: int = 20) -> List[Task]:
        """å…¨æ–‡æœç´¢ä¼˜åŒ–"""
        # ä½¿ç”¨FTSç´¢å¼•è¿›è¡Œå…¨æ–‡æœç´¢
        fts_results = self.db.execute(text("""
            SELECT task.id FROM task_fts
            JOIN task ON task.id = task_fts.rowid
            WHERE task_fts MATCH :search_term
            LIMIT :limit
        """), {"search_term": search_term, "limit": limit})
        
        task_ids = [row[0] for row in fts_results]
        
        if not task_ids:
            return []
        
        # æ‰¹é‡åŠ è½½åŒ¹é…çš„ä»»åŠ¡
        return self.db.query(Task).filter(Task.id.in_(task_ids)).all()
```

#### 5.2.2 ç¼“å­˜ç­–ç•¥
```python
from functools import wraps
import redis
import pickle
from typing import Any, Callable

class CacheManager:
    def __init__(self):
        self.redis_client = redis.Redis(
            host=settings.REDIS_HOST,
            port=settings.REDIS_PORT,
            db=settings.REDIS_DB
        )
    
    def cache_result(self, key_prefix: str, ttl: int = 3600):
        """ç¼“å­˜è£…é¥°å™¨"""
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            async def wrapper(*args, **kwargs) -> Any:
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = f"{key_prefix}:{hash(str(args) + str(kwargs))}"
                
                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = self.redis_client.get(cache_key)
                if cached_result:
                    return pickle.loads(cached_result)
                
                # æ‰§è¡Œå‡½æ•°
                result = await func(*args, **kwargs)
                
                # å­˜å‚¨åˆ°ç¼“å­˜
                self.redis_client.setex(
                    cache_key, 
                    ttl, 
                    pickle.dumps(result)
                )
                
                return result
            return wrapper
        return decorator

cache_manager = CacheManager()

# ä½¿ç”¨ç¼“å­˜
@cache_manager.cache_result("task_list", ttl=1800)
async def get_tasks_cached(filters: dict = None):
    return await TaskRepository(db).get_tasks_with_relations(filters)

@cache_manager.cache_result("ai_classification", ttl=3600)
async def classify_task_cached(task_content: str, context: dict):
    return await ai_service.classify_task(task_content, context)
```

#### 5.2.3 å¼‚æ­¥ä»»åŠ¡å¤„ç†
```python
from celery import Celery
from typing import Dict, Any

# Celeryé…ç½®
celery_app = Celery(
    'taskwall',
    broker='redis://localhost:6379/1',
    backend='redis://localhost:6379/1'
)

@celery_app.task
async def process_ai_enhancement(task_id: int, enhancement_type: str):
    """å¼‚æ­¥AIå¢å¼ºå¤„ç†"""
    try:
        db = SessionLocal()
        task = db.query(Task).filter(Task.id == task_id).first()
        
        if not task:
            return {"error": "Task not found"}
        
        ai_service = AIServiceAggregator()
        
        if enhancement_type == "classification":
            result = await ai_service.classification_service.process_with_cache({
                "task_data": task.__dict__,
                "user_context": {}
            })
            
            # æ›´æ–°ä»»åŠ¡åˆ†ç±»
            task.category = result["classification"]["category"]
            task.ai_confidence = result["classification"]["confidence"]
            
        elif enhancement_type == "priority":
            result = await ai_service.priority_service.process_with_cache({
                "task_data": task.__dict__,
                "context": {}
            })
            
            # æ›´æ–°ä¼˜å…ˆçº§å»ºè®®
            task.ai_suggested_priority = result["suggested_priority"]
            task.ai_confidence = result["confidence"]
        
        db.commit()
        return {"success": True, "task_id": task_id}
        
    except Exception as e:
        db.rollback()
        return {"error": str(e)}
    finally:
        db.close()

@celery_app.task
async def batch_similarity_update():
    """æ‰¹é‡æ›´æ–°ç›¸ä¼¼åº¦ç´¢å¼•"""
    try:
        db = SessionLocal()
        vector_db = VectorDBManager()
        
        # è·å–éœ€è¦æ›´æ–°çš„ä»»åŠ¡
        tasks_to_update = db.query(Task).filter(
            Task.updated_at > Task.last_vector_update
        ).all()
        
        # æ‰¹é‡æ›´æ–°å‘é‡
        for task in tasks_to_update:
            content = f"{task.title} {task.description}"
            await vector_db.update_task_vector(task.id, content, {
                "title": task.title,
                "category": task.category,
                "priority": task.priority
            })
            
            task.last_vector_update = datetime.utcnow()
        
        db.commit()
        return {"updated_count": len(tasks_to_update)}
        
    except Exception as e:
        db.rollback()
        return {"error": str(e)}
    finally:
        db.close()
```

---

## 6. éƒ¨ç½²æ¶æ„

### 6.1 Dockerå®¹å™¨åŒ–

#### 6.1.1 Dockerfileè®¾è®¡
```dockerfile
# å‰ç«¯Dockerfile
FROM node:18-alpine AS frontend-builder

WORKDIR /app
COPY frontend/package*.json ./
RUN npm ci --only=production

COPY frontend/ .
RUN npm run build

# åç«¯Dockerfile  
FROM python:3.11-slim AS backend

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Pythonä¾èµ–
COPY backend/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY backend/ .

# åˆ›å»ºæ•°æ®ç›®å½•
RUN mkdir -p /app/data /app/logs

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV UVICORN_HOST=0.0.0.0
ENV UVICORN_PORT=8000

EXPOSE 8000

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

# Nginx Dockerfile
FROM nginx:alpine AS frontend

COPY --from=frontend-builder /app/dist /usr/share/nginx/html
COPY nginx.conf /etc/nginx/nginx.conf

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
```

#### 6.1.2 Docker Composeé…ç½®
```yaml
version: '3.8'

services:
  # å‰ç«¯æœåŠ¡
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    ports:
      - "3000:80"
    depends_on:
      - backend
    environment:
      - API_BASE_URL=http://backend:8000
    networks:
      - taskwall-network

  # åç«¯æœåŠ¡
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    ports:
      - "8000:8000"
    depends_on:
      - redis
      - chromadb
    environment:
      - DATABASE_URL=sqlite:///./data/taskwall.db
      - REDIS_URL=redis://redis:6379/0
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    networks:
      - taskwall-network

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    networks:
      - taskwall-network

  # ChromaDBå‘é‡æ•°æ®åº“
  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma-data:/chroma/chroma
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_HTTP_PORT=8000
    networks:
      - taskwall-network

  # Celery Worker (å¼‚æ­¥ä»»åŠ¡å¤„ç†)
  celery-worker:
    build:
      context: .
      dockerfile: Dockerfile.backend
    command: celery -A app.celery worker --loglevel=info
    depends_on:
      - redis
      - backend
    environment:
      - DATABASE_URL=sqlite:///./data/taskwall.db
      - REDIS_URL=redis://redis:6379/1
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    networks:
      - taskwall-network

  # Celery Beat (å®šæ—¶ä»»åŠ¡)
  celery-beat:
    build:
      context: .
      dockerfile: Dockerfile.backend
    command: celery -A app.celery beat --loglevel=info
    depends_on:
      - redis
      - backend
    environment:
      - DATABASE_URL=sqlite:///./data/taskwall.db
      - REDIS_URL=redis://redis:6379/1
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    networks:
      - taskwall-network

volumes:
  redis-data:
  chroma-data:

networks:
  taskwall-network:
    driver: bridge
```

### 6.2 ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

#### 6.2.1 Kuberneteséƒ¨ç½²é…ç½®
```yaml
# namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: taskwall

---
# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: taskwall-config
  namespace: taskwall
data:
  DATABASE_URL: "sqlite:///./data/taskwall.db"
  REDIS_URL: "redis://redis-service:6379/0"
  CHROMA_HOST: "chromadb-service"
  CHROMA_PORT: "8000"

---
# secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: taskwall-secrets
  namespace: taskwall
type: Opaque
data:
  GEMINI_API_KEY: <base64-encoded-api-key>

---
# backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: taskwall-backend
  namespace: taskwall
spec:
  replicas: 2
  selector:
    matchLabels:
      app: taskwall-backend
  template:
    metadata:
      labels:
        app: taskwall-backend
    spec:
      containers:
      - name: backend
        image: taskwall/backend:v3.0
        ports:
        - containerPort: 8000
        envFrom:
        - configMapRef:
            name: taskwall-config
        - secretRef:
            name: taskwall-secrets
        volumeMounts:
        - name: data-volume
          mountPath: /app/data
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
      volumes:
      - name: data-volume
        persistentVolumeClaim:
          claimName: taskwall-data-pvc

---
# backend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: taskwall-backend-service
  namespace: taskwall
spec:
  selector:
    app: taskwall-backend
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
  type: ClusterIP

---
# ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: taskwall-ingress
  namespace: taskwall
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - taskwall.example.com
    secretName: taskwall-tls
  rules:
  - host: taskwall.example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: taskwall-backend-service
            port:
              number: 8000
      - path: /
        pathType: Prefix
        backend:
          service:
            name: taskwall-frontend-service
            port:
              number: 80
```

---

## 7. ç›‘æ§ä¸æ—¥å¿—

### 7.1 ç›‘æ§ä½“ç³»

#### 7.1.1 PrometheusæŒ‡æ ‡æ”¶é›†
```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# å®šä¹‰ç›‘æ§æŒ‡æ ‡
request_count = Counter('taskwall_requests_total', 'Total requests', ['method', 'endpoint'])
request_duration = Histogram('taskwall_request_duration_seconds', 'Request duration')
ai_processing_time = Histogram('taskwall_ai_processing_seconds', 'AI processing time', ['operation'])
active_tasks = Gauge('taskwall_active_tasks', 'Number of active tasks')
ai_accuracy = Gauge('taskwall_ai_accuracy', 'AI operation accuracy', ['operation'])

class MetricsMiddleware:
    def __init__(self, app):
        self.app = app
    
    async def __call__(self, scope, receive, send):
        if scope["type"] == "http":
            method = scope["method"]
            path = scope["path"]
            
            start_time = time.time()
            
            # å¢åŠ è¯·æ±‚è®¡æ•°
            request_count.labels(method=method, endpoint=path).inc()
            
            # æ‰§è¡Œè¯·æ±‚
            await self.app(scope, receive, send)
            
            # è®°å½•è¯·æ±‚è€—æ—¶
            duration = time.time() - start_time
            request_duration.observe(duration)
        else:
            await self.app(scope, receive, send)

# AIæœåŠ¡ç›‘æ§
class AIMetricsCollector:
    @staticmethod
    def record_ai_operation(operation: str, duration: float, accuracy: float = None):
        ai_processing_time.labels(operation=operation).observe(duration)
        if accuracy is not None:
            ai_accuracy.labels(operation=operation).set(accuracy)
    
    @staticmethod
    def update_task_count(count: int):
        active_tasks.set(count)

# å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨
start_http_server(8001)
```

#### 7.1.2 å¥åº·æ£€æŸ¥ç«¯ç‚¹
```python
from fastapi import HTTPException
from sqlalchemy import text

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "services": {}
    }
    
    try:
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        db = SessionLocal()
        db.execute(text("SELECT 1"))
        health_status["services"]["database"] = "healthy"
        db.close()
    except Exception as e:
        health_status["services"]["database"] = f"unhealthy: {str(e)}"
        health_status["status"] = "unhealthy"
    
    try:
        # æ£€æŸ¥Redisè¿æ¥
        redis_client = redis.Redis()
        redis_client.ping()
        health_status["services"]["redis"] = "healthy"
    except Exception as e:
        health_status["services"]["redis"] = f"unhealthy: {str(e)}"
        health_status["status"] = "unhealthy"
    
    try:
        # æ£€æŸ¥ChromaDBè¿æ¥
        chroma_client = chromadb.Client()
        chroma_client.heartbeat()
        health_status["services"]["chromadb"] = "healthy"
    except Exception as e:
        health_status["services"]["chromadb"] = f"unhealthy: {str(e)}"
        health_status["status"] = "unhealthy"
    
    if health_status["status"] == "unhealthy":
        raise HTTPException(status_code=503, detail=health_status)
    
    return health_status

@app.get("/metrics")
async def get_custom_metrics():
    """è‡ªå®šä¹‰æŒ‡æ ‡ç«¯ç‚¹"""
    db = SessionLocal()
    
    try:
        # ç»Ÿè®¡ä»»åŠ¡æ•°é‡
        total_tasks = db.query(Task).count()
        active_tasks_count = db.query(Task).filter(Task.status != TaskStatus.DONE).count()
        
        # ç»Ÿè®¡AIä½¿ç”¨æƒ…å†µ
        ai_enhanced_tasks = db.query(Task).filter(Task.ai_confidence > 0).count()
        
        # ç»Ÿè®¡ä¾èµ–å…³ç³»
        total_dependencies = db.query(TaskDependency).count()
        ai_inferred_deps = db.query(TaskDependency).filter(TaskDependency.ai_inferred == True).count()
        
        return {
            "tasks": {
                "total": total_tasks,
                "active": active_tasks_count,
                "ai_enhanced": ai_enhanced_tasks
            },
            "dependencies": {
                "total": total_dependencies,
                "ai_inferred": ai_inferred_deps
            },
            "ai_usage": {
                "enhancement_rate": ai_enhanced_tasks / total_tasks if total_tasks > 0 else 0,
                "dependency_inference_rate": ai_inferred_deps / total_dependencies if total_dependencies > 0 else 0
            }
        }
    finally:
        db.close()
```

### 7.2 æ—¥å¿—ç®¡ç†

#### 7.2.1 ç»“æ„åŒ–æ—¥å¿—é…ç½®
```python
import structlog
import logging.config

# é…ç½®ç»“æ„åŒ–æ—¥å¿—
logging.config.dictConfig({
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "json": {
            "()": structlog.stdlib.ProcessorFormatter,
            "processor": structlog.dev.ConsoleRenderer(colors=False),
        },
    },
    "handlers": {
        "console": {
            "class": "logging.StreamHandler",
            "formatter": "json",
            "stream": "ext://sys.stdout",
        },
        "file": {
            "class": "logging.handlers.RotatingFileHandler",
            "formatter": "json",
            "filename": "/app/logs/taskwall.log",
            "maxBytes": 10485760,  # 10MB
            "backupCount": 5,
        },
    },
    "loggers": {
        "": {
            "handlers": ["console", "file"],
            "level": "INFO",
        },
        "taskwall.ai": {
            "handlers": ["console", "file"],
            "level": "DEBUG",
            "propagate": False,
        },
    },
})

structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

# è·å–ç»“æ„åŒ–æ—¥å¿—å™¨
logger = structlog.get_logger("taskwall.ai")

# ä½¿ç”¨ç¤ºä¾‹
logger.info("AI operation started", 
           operation="task_classification",
           task_id=123,
           user_id="user_456")

logger.error("AI operation failed",
            operation="dependency_inference", 
            error="API timeout",
            task_count=5,
            duration=30.5)
```

#### 7.2.2 æ—¥å¿—èšåˆé…ç½®
```yaml
# Fluentdé…ç½® (fluent.conf)
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

<filter taskwall.**>
  @type parser
  key_name log
  <parse>
    @type json
  </parse>
</filter>

<match taskwall.**>
  @type elasticsearch
  host elasticsearch
  port 9200
  index_name taskwall-logs
  type_name _doc
  logstash_format true
  logstash_prefix taskwall
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  tag_key @log_name
  flush_interval 1s
</match>
```

---

## 8. å®‰å…¨æ¶æ„

### 8.1 è®¤è¯ä¸æˆæƒ

#### 8.1.1 JWTä»¤ç‰Œç®¡ç†
```python
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt
from datetime import datetime, timedelta

security = HTTPBearer()

class AuthManager:
    def __init__(self):
        self.secret_key = settings.JWT_SECRET_KEY
        self.algorithm = "HS256"
        self.access_token_expire = timedelta(hours=24)
        self.refresh_token_expire = timedelta(days=7)
    
    def create_access_token(self, user_id: str, permissions: List[str] = None) -> str:
        """åˆ›å»ºè®¿é—®ä»¤ç‰Œ"""
        expire = datetime.utcnow() + self.access_token_expire
        payload = {
            "user_id": user_id,
            "permissions": permissions or [],
            "exp": expire,
            "type": "access"
        }
        return jwt.encode(payload, self.secret_key, algorithm=self.algorithm)
    
    def verify_token(self, token: str) -> dict:
        """éªŒè¯ä»¤ç‰Œ"""
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])
            if payload.get("type") != "access":
                raise HTTPException(status_code=401, detail="Invalid token type")
            return payload
        except jwt.ExpiredSignatureError:
            raise HTTPException(status_code=401, detail="Token expired")
        except jwt.JWTError:
            raise HTTPException(status_code=401, detail="Invalid token")

auth_manager = AuthManager()

async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)) -> dict:
    """è·å–å½“å‰ç”¨æˆ·"""
    token = credentials.credentials
    payload = auth_manager.verify_token(token)
    return {
        "user_id": payload["user_id"],
        "permissions": payload["permissions"]
    }

def require_permission(permission: str):
    """æƒé™æ£€æŸ¥è£…é¥°å™¨"""
    def decorator(func):
        async def wrapper(*args, current_user: dict = Depends(get_current_user), **kwargs):
            if permission not in current_user["permissions"]:
                raise HTTPException(
                    status_code=403, 
                    detail=f"Permission required: {permission}"
                )
            return await func(*args, current_user=current_user, **kwargs)
        return wrapper
    return decorator
```

### 8.2 æ•°æ®å®‰å…¨

#### 8.2.1 æ•æ„Ÿæ•°æ®åŠ å¯†
```python
from cryptography.fernet import Fernet
import base64
import hashlib

class DataEncryption:
    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è·å–åŠ å¯†å¯†é’¥
        self.key = self._derive_key(settings.ENCRYPTION_SECRET)
        self.cipher = Fernet(self.key)
    
    def _derive_key(self, secret: str) -> bytes:
        """ä»å¯†ç æ´¾ç”ŸåŠ å¯†å¯†é’¥"""
        key = hashlib.pbkdf2_hmac('sha256', secret.encode(), b'salt', 100000)
        return base64.urlsafe_b64encode(key)
    
    def encrypt(self, data: str) -> str:
        """åŠ å¯†å­—ç¬¦ä¸²"""
        if not data:
            return data
        encrypted_data = self.cipher.encrypt(data.encode())
        return base64.urlsafe_b64encode(encrypted_data).decode()
    
    def decrypt(self, encrypted_data: str) -> str:
        """è§£å¯†å­—ç¬¦ä¸²"""
        if not encrypted_data:
            return encrypted_data
        try:
            encrypted_bytes = base64.urlsafe_b64decode(encrypted_data.encode())
            decrypted_data = self.cipher.decrypt(encrypted_bytes)
            return decrypted_data.decode()
        except Exception:
            return ""  # è§£å¯†å¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²

# åœ¨æ•°æ®æ¨¡å‹ä¸­ä½¿ç”¨åŠ å¯†
from sqlalchemy_utils import EncryptedType
from sqlalchemy_utils.types.encrypted.encrypted_type import AesEngine

class SensitiveTask(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    title: str
    # æ•æ„Ÿæè¿°å­—æ®µåŠ å¯†å­˜å‚¨
    encrypted_description: str = Field(sa_column=Column(EncryptedType(String, settings.ENCRYPTION_SECRET, AesEngine, 'pkcs5')))
    
    @property
    def description(self) -> str:
        return self.encrypted_description
    
    @description.setter
    def description(self, value: str):
        self.encrypted_description = value
```

#### 8.2.2 APIå®‰å…¨ä¸­é—´ä»¶
```python
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response
import time

class SecurityMiddleware(BaseHTTPMiddleware):
    def __init__(self, app):
        super().__init__(app)
        self.rate_limiter = RateLimiter()
    
    async def dispatch(self, request: Request, call_next):
        # 1. é€Ÿç‡é™åˆ¶
        client_ip = self._get_client_ip(request)
        if not await self.rate_limiter.allow_request(client_ip):
            return Response(
                content="Too many requests",
                status_code=429
            )
        
        # 2. å®‰å…¨å¤´æ£€æŸ¥
        if not self._check_security_headers(request):
            return Response(
                content="Security headers required",
                status_code=400
            )
        
        # 3. å¤„ç†è¯·æ±‚
        response = await call_next(request)
        
        # 4. æ·»åŠ å®‰å…¨å“åº”å¤´
        self._add_security_headers(response)
        
        return response
    
    def _get_client_ip(self, request: Request) -> str:
        """è·å–å®¢æˆ·ç«¯IPåœ°å€"""
        forwarded = request.headers.get("X-Forwarded-For")
        if forwarded:
            return forwarded.split(",")[0].strip()
        return request.client.host
    
    def _check_security_headers(self, request: Request) -> bool:
        """æ£€æŸ¥å®‰å…¨å¤´"""
        # æ£€æŸ¥CSRFä»¤ç‰Œ
        if request.method in ["POST", "PUT", "DELETE", "PATCH"]:
            csrf_token = request.headers.get("X-CSRF-Token")
            if not csrf_token:
                return False
        return True
    
    def _add_security_headers(self, response: Response):
        """æ·»åŠ å®‰å…¨å“åº”å¤´"""
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-Frame-Options"] = "DENY"
        response.headers["X-XSS-Protection"] = "1; mode=block"
        response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
        response.headers["Content-Security-Policy"] = "default-src 'self'"

class RateLimiter:
    def __init__(self):
        self.redis_client = redis.Redis()
        self.max_requests = 100  # æ¯åˆ†é’Ÿæœ€å¤š100ä¸ªè¯·æ±‚
        self.window_size = 60    # æ—¶é—´çª—å£60ç§’
    
    async def allow_request(self, client_ip: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚"""
        key = f"rate_limit:{client_ip}"
        current_time = int(time.time())
        window_start = current_time - self.window_size
        
        # ä½¿ç”¨æ»‘åŠ¨çª—å£ç®—æ³•
        pipe = self.redis_client.pipeline()
        pipe.zremrangebyscore(key, 0, window_start)
        pipe.zcard(key)
        pipe.zadd(key, {str(current_time): current_time})
        pipe.expire(key, self.window_size)
        
        results = pipe.execute()
        current_requests = results[1]
        
        return current_requests < self.max_requests

# åº”ç”¨ä¸­é—´ä»¶
app.add_middleware(SecurityMiddleware)
```

---

## 9. æ€»ç»“

### 9.1 æ¶æ„ä¼˜åŠ¿

1. **æ¨¡å—åŒ–è®¾è®¡**: æ¸…æ™°çš„åˆ†å±‚æ¶æ„ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
2. **AIé©±åŠ¨**: æ·±åº¦é›†æˆAIåŠŸèƒ½ï¼Œæä¾›æ™ºèƒ½åŒ–ä»»åŠ¡ç®¡ç†
3. **é«˜æ€§èƒ½**: å¤šå±‚ç¼“å­˜ã€å¼‚æ­¥å¤„ç†ã€æ•°æ®åº“ä¼˜åŒ–
4. **å¯æ‰©å±•**: å¾®æœåŠ¡æ¶æ„ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•
5. **å®‰å…¨å¯é **: å®Œæ•´çš„å®‰å…¨æªæ–½å’Œç›‘æ§ä½“ç³»

### 9.2 æŠ€æœ¯é€‰å‹åˆç†æ€§

- **Vue 3 + TypeScript**: ç°ä»£å‰ç«¯æŠ€æœ¯æ ˆï¼Œå¼€å‘æ•ˆç‡é«˜
- **FastAPI**: é«˜æ€§èƒ½å¼‚æ­¥Webæ¡†æ¶ï¼Œè‡ªåŠ¨APIæ–‡æ¡£
- **SQLite + ChromaDB**: è½»é‡çº§éƒ¨ç½²ï¼Œå‘é‡æœç´¢èƒ½åŠ›å¼º
- **Redis**: é«˜æ€§èƒ½ç¼“å­˜ï¼Œæ”¯æŒå¤šç§æ•°æ®ç»“æ„
- **Docker**: å®¹å™¨åŒ–éƒ¨ç½²ï¼Œç¯å¢ƒä¸€è‡´æ€§

### 9.3 é£é™©æ§åˆ¶

- **AIä¾èµ–**: å¤šä¸ªAIæœåŠ¡æä¾›å•†ï¼Œé™ä½å•ç‚¹æ•…éšœé£é™©
- **æ€§èƒ½ç“¶é¢ˆ**: å®Œå–„çš„ç¼“å­˜ç­–ç•¥å’Œå¼‚æ­¥å¤„ç†
- **æ•°æ®å®‰å…¨**: å¤šå±‚æ¬¡å®‰å…¨æªæ–½ï¼ŒåŠ å¯†å­˜å‚¨
- **å¯ç”¨æ€§**: å¥åº·æ£€æŸ¥ã€ç›‘æ§å‘Šè­¦ã€æ•…éšœæ¢å¤

### 9.4 åç»­ä¼˜åŒ–æ–¹å‘

1. **å¾®æœåŠ¡æ‹†åˆ†**: éšç€ä¸šåŠ¡å¢é•¿ï¼Œè€ƒè™‘è¿›ä¸€æ­¥æ‹†åˆ†æœåŠ¡
2. **æœºå™¨å­¦ä¹ ä¼˜åŒ–**: å¼•å…¥æ›´å…ˆè¿›çš„MLæ¨¡å‹
3. **å¤šç§Ÿæˆ·æ”¯æŒ**: ä¼ä¸šçº§å¤šç§Ÿæˆ·æ¶æ„
4. **å›½é™…åŒ–**: å¤šè¯­è¨€å’Œå¤šåœ°åŒºæ”¯æŒ

---

**æ–‡æ¡£ç»´æŠ¤**: æœ¬æŠ€æœ¯æ¶æ„æ–‡æ¡£å°†éšç€ç³»ç»Ÿæ¼”è¿›æŒç»­æ›´æ–°ï¼Œç¡®ä¿æ¶æ„è®¾è®¡ä¸å®é™…å®ç°ä¿æŒä¸€è‡´ã€‚